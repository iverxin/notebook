[TOC]

# 1. C4.5

## 1.1 Feature：

1. 有监督学习
2. ID3的一种决策树诱导算法。ID3被称为迭代分解器(iterative dichotomizers)第三代。

3. 除了诱导出决策树，还可以将决策树转换成具有良好理解性的规则。
4. 通过C4.5的后剪枝操作得到的分类器不能在精确地被转换会决策树。

### Advantage

- 良好的可理解性的规则
- 剪枝能力
- 计算复杂度不高
- 中间值缺失不敏感
- 可以处理不相关特征数据

### Disadvantage
- 可能产生过度匹配的问题


## 1.2 Describe

遵循传统的递归模式，首先用根节点表示一定量的数据集，从根节点开始每个结点测试一个特定的属性(该属性有算法确定)，根据这个属性将数据集划分成更小的子集，用子树表示。直到子集中所有的数据集都是同一类别，该节点停止分裂。



### 决策树剪枝

目的：解决过拟合的问题。通常在树全部生成之后进行，而且采用自下而上的方式。

- 基于成本复杂度的剪枝(cost-complexity pruning)

- 错误消减剪枝(Reduce error pruning)是上个方法的简化版本

- 悲观剪枝(Pessimistic pruning)，C4.5算法创造性的提出。

- 基于理想置信区间剪枝(confidence intervals, CI)，悲观剪枝的扩展

### 连续型属性

### 缺失值处理

### 规则集诱导

## Theory

### 递归伪代码

```python
#createBranch函数:递归
检测当前数据集中每个example是否是同一个类:
	if 是同一个类 or 实例总数低于某阀值 
    	return 类标签
	else
		寻找划分该数据集的最好的feature
		划分数据集
		创建分支结点
			for 每个划分的子集
				调用createBranch并增加返回结果到分支节点中 #递归调用自己
		return 分支节点
```

### 每个结点的特征选取

使用`信息熵增益(gain)`和`增益率(gain ratio)` 。

`信息熵增益`准则的一个缺陷在于过于偏向选择具有更多输出结果的测试

`增益率`能够克服`信息熵增益`的短板。C4.5默认使用`信息熵增益率`。

#### 信息熵计算

对于不相关的时间x和y，同时发生的信息熵就是他们两个的和：

$$ H(x,y)=H(x)+H(y)$$

> $p(x,y)=p(x)p(y)$ 两事件不相关

所以，对于n个事件，同时发生的信息熵为。

**$$ H(x)=-\sum^n_{i=1} p(x_i)log(p(x_i))$$**

> $p(x_i)$代表x事件的概率。机器学习里可以表某个类的概率
>
> n为分类的数目

- 概率越小的所含有的信息量越大。

> 概率发生小的事情人比较震撼，百分之百发生的事情人的反应比较小。

- 有log的原因是log可以使让累加计算变成真数相乘。选择2是普遍传统，可以自己改。



#### 信息熵增益

某属性下两次信息熵的差值。信息熵的下降幅度。信息熵增益越大，说明信息熵的下降幅度越高，特征选择的效果越好，纯净度越高。

划分之前的信息熵( H(D) )减去划分之后**每个子集**的信息熵**加权平均**。



$$Gain(D,a)=H(D)-\sum^V_{v=1} \frac{|D^v|}{|D|}H(D^v)$$

> H(D) 该节点的信息熵增益
>
> $|D_v|$ : D结点的分出来的第V个类所含有的example数量
>
> $|D|$: D结点总的example数量
>
> $\frac{|D^v|}{|D|}$: v类在D结点中的概率
>
> $H(D_v)$: v类的信息熵增益

**缺点**：会倾向选择那些属性取向值较多的属性。

分析：当上述公式中v的种类很多，导致每个类中example很小，也就是说$|D_v|$很小时，$\frac{D^v}{|D|}$趋近于0，此时如果v类中的纯净度还很高，$H(D^v)$也趋近于0。所以此时信息上增益最大。但是这种划分可能毫无意义。

> 例如极端情况，以学生ID作为分类的属性，ID可以有很多取值。由于每个学生ID都是唯一的。所以$H(D^v)$是0。此时计算出来的Gain(D,a)是最大的。按照要求选择ID属性划分很理想，但是这种划分并不能说明什么问题，没有意义。

#### 信息熵增益率

用来解决信息熵增益的选择倾向问题。

$GainRatio(a)=\frac{Gain(a)}{Entropy(a)}$

> Entropy(a): a的熵
>
> Gain(a): a的信息增益

**缺点：**信息熵的增益率对属性取值较少的属性有所偏好。

> C4.5并不是直接选择增益率最大的属性作为划分属性，而是之前先通过一遍筛选，先把信息增益低于平均水平的属性剔除掉，之后从剩下的属性中选择信息增益率最高的，这样的话，相当于两方面都得到了兼顾。 

### 如何确定叶节点类

所占数量最多的。

## 2.1 Scope of application

数据行和标称型
- 处理分类问题
- 专家系统



# 2. CART

## Describe

CART(Classification And Regression Trees) 分类回归树。即可以用于分类，也可以用于回归问题。

注意：CART是**二叉树**

```python
# createTree
找到最佳的待切分特征:
    如果该节点不能再分，将该节点存为叶节点
    执行二元切分
    左子树调用createTree()方法
    右子树调用createTree()方法
```

CART除了可以用来分类，使用Gini来进行划分。还可以用来回归。

当数据有众多特征且特征之间关系复杂时，一个回归模型可能很难很好的很好描绘所有的样本。而且对于非线性的问题，也很难用线性回归模型处理。所以可以把整个问题切分成许多小的段，每个段都有自己的回归函数。这种思想的指引下，利用树就是一个很好的方向。回归树和模型树由此而来。

- 回归树的结点是数值。

- 模型树的结点是线性方程。



## Threoy

### 决策树：

#### Gini指数的概念

Gini是一种不等性的度量，比如用来度量收入不平衡。

取值返回是0~1，0代表完全相等，1代表完全不相等。总体内部包含的类别越杂乱，Gini指数越大。类似熵的概念

$$Gini(T)=1-\sum_{j=1}^{n}p_j^2$$

> n代表T集合中的类别数
>
> $p_j$代表该类别的概率

#### Gini指数增益（子集划分原理）

$GiniGain(T)=\sum{\frac{N_i}{N}Gini(T_i)}$

> i：特征的第i个取值。

对于二分法，选取某个特征，会将集合T分成两个子集合，T1和T2。

则该特征的GiniGain(Gini增益)

$GiniGain(T)=\frac{N_1}{N}Gain(T_1)+\frac{N_2}{N}Gain(T2)$

>Ni为Ti集合中example数量
>
>N为T集合中example数量

计算每个特征的GiniGain，选择最**小**的那个特征的GiniGain。

### 回归树





### 模型树



## Others

### Difference of C4.5/ID3/CART

- ID3决策树利用信息熵增益来划分结点
- C4.5决策树：先算信息熵增益，去掉低于平均值的，然后取增益率最高的
- CART决策树(Classification And Regression Tree)：可用于分类和回归，划分结点基于GINI

ID3 ：对于连续变量必须进行离散化，但是离散化后的变量会失去一部分联系。

### Advantages of Tree Regression

- 可以对复杂和非线性的数据建模。

# 2. K-means

## Feature：

### Advantage：

### Disadvantage：





# 3. SVM



# 4. Apriori



# 5. EM 数学太多

## Describe

期望最大化(EM)算法是一种被广泛用于极大似然(ML)估计的迭代性型计算方法。处理大量数据不完整问题非常有用。

Advantages：

-　数值计算的稳定
-　实现简单
-　可靠全局收敛

# 6. PageRank 网搜



# 7. AdaBoost



# 8. KNN

## 8.1 Feature

采用测量不同特征值之间的距离方法来进行分类。



### Advantage

- 精度高
- 对异常值不敏感
- 无数据输入假定

### Disadvantage

- 计算复杂度高、空间复杂度高

- 无法给出数据的内在含义

## 8.2 Scope of application

数值型和标称型



## 8.3 Theory

存在**带标签**的训练样本集，输入没有标签的新数据后，`新数据`的每个特征与`样本集`中数据对应的特征进行比较，然后提取样本集中特征最相似的分类的标签。一般只选择前k个最相似的数据，所以叫做k-邻近算法。

>  example：
>
> 确定一个电影是动作片还是爱情片，已知数据集特征：打斗场面次数、接吻镜头次数。给出新的未标签的电影，计算该电影与数据集中每个电影的距离，然后排序取前k个，通过分析k个电影的种类来确定新的未标签的电影类型。



# 9. Naive Bayes



- 