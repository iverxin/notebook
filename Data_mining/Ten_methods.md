[TOC]

# 1. C4.5

## 1.1 Feature：

1. 有监督学习
2. ID3的一种决策树诱导算法。ID3被称为迭代分解器(iterative dichotomizers)第三代。

3. 除了诱导出决策树，还可以将决策树转换成具有良好理解性的规则。
4. 通过C4.5的后剪枝操作得到的分类器不能在精确地被转换会决策树。

### Advantage

- 良好的可理解性的规则
- 剪枝能力
- 计算复杂度不高
- 中间值缺失不敏感
- 可以处理不相关特征数据

### Disadvantage
- 可能产生过度匹配的问题


## 1.2 Describe

遵循传统的递归模式，首先用根节点表示一定量的数据集，从根节点开始每个结点测试一个特定的属性(该属性有算法确定)，根据这个属性将数据集划分成更小的子集，用子树表示。直到子集中所有的数据集都是同一类别，该节点停止分裂。



### 决策树剪枝

目的：解决过拟合的问题。通常在树全部生成之后进行，而且采用自下而上的方式。

- 基于成本复杂度的剪枝(cost-complexity pruning)

- 错误消减剪枝(Reduce error pruning)是上个方法的简化版本

- 悲观剪枝(Pessimistic pruning)，C4.5算法创造性的提出。

- 基于理想置信区间剪枝(confidence intervals, CI)，悲观剪枝的扩展

### 连续型属性

### 缺失值处理

### 规则集诱导

## Theory

### 递归伪代码

```python
#createBranch函数:递归
检测当前数据集中每个example是否是同一个类:
	if 是同一个类 or 实例总数低于某阀值 
    	return 类标签
	else
		寻找划分该数据集的最好的feature
		划分数据集
		创建分支结点
			for 每个划分的子集
				调用createBranch并增加返回结果到分支节点中 #递归调用自己
		return 分支节点
```

### 每个结点的特征选取：

使用`信息熵增益(gain)`和`增益率(gain ratio)` 。

`信息熵增益`准则的一个缺陷在于过于偏向选择具有更多输出结果的测试

`增益率`能够克服`信息熵增益`的短板。C4.5默认使用`信息熵增益率`。

#### 信息熵计算：

对于不相关的时间x和y，同时发生的信息熵就是他们两个的和：

$$ H(x,y)=H(x)+H(y)$$

> $p(x,y)=p(x)p(y)$ 两事件不相关

所以，对于n个事件，同时发生的信息熵为。

**$$ H(x)=-\sum^n_{i=1} p(x_i)log(p(x_i))$$**

> $p(x_i)$代表x事件的概率。机器学习里可以表某个类的概率
>
> n为分类的数目

- 概率越小的所含有的信息量越大。

> 概率发生小的事情人比较震撼，百分之百发生的事情人的反应比较小。

- 有log的原因是log可以使让累加计算变成真数相乘。选择2是普遍传统，可以自己改。



#### 信息熵增益

某属性下两次信息熵的差值。信息熵的下降幅度。信息熵增益越大，说明信息熵的下降幅度越高，特征选择的效果越好，纯净度越高。

划分之前的信息熵( H(D) )减去划分之后**每个子集**的信息熵**加权平均**。



$$Gain(D,a)=H(D)-\sum^V_{v=1} \frac{|D^v|}{|D|}H(D^v)$$

> H(D) 该节点的信息熵增益
>
> $|D_v|$ : D结点的分出来的第V个类所含有的example数量
>
> $|D|$: D结点总的example数量
>
> $\frac{|D^v|}{|D|}$: v类在D结点中的概率
>
> $H(D_v)$: v类的信息熵增益

**缺点**：会倾向选择那些属性取向值较多的属性。

分析：当上述公式中v的种类很多，导致每个类中example很小，也就是说$|D_v|$很小时，$\frac{D^v}{|D|}$趋近于0，此时如果v类中的纯净度还很高，$H(D^v)$也趋近于0。所以此时信息上增益最大。但是这种划分可能毫无意义。

> 例如极端情况，以学生ID作为分类的属性，ID可以有很多取值。由于每个学生ID都是唯一的。所以$H(D^v)$是0。此时计算出来的Gain(D,a)是最大的。按照要求选择ID属性划分很理想，但是这种划分并不能说明什么问题，没有意义。

#### 信息熵增益率

用来解决信息熵增益的选择倾向问题。

$GainRatio(a)=\frac{Gain(a)}{Entropy(a)}$

> Entropy(a): a的熵
>
> Gain(a): a的信息增益

**缺点：**信息熵的增益率对属性取值较少的属性有所偏好。

> C4.5并不是直接选择增益率最大的属性作为划分属性，而是之前先通过一遍筛选，先把信息增益低于平均水平的属性剔除掉，之后从剩下的属性中选择信息增益率最高的，这样的话，相当于两方面都得到了兼顾。 

### 如何确定叶节点类：

所占数量最多的。

## 2.1 Scope of application

数据行和标称型
- 处理分类问题
- 专家系统


# 2. K-means

## Feature：

### Advantage：

### Disadvantage：





# 3. SVM



# 4. Apriori



# 5. EM 数学太多

## Describe

期望最大化(EM)算法是一种被广泛用于极大似然估计

# 6. PageRank 网搜



# 7. AdaBoost



# 8. KNN

## 8.1 Feature

采用测量不同特征值之间的距离方法来进行分类。



### Advantage

- 精度高
- 对异常值不敏感
- 无数据输入假定

### Disadvantage

- 计算复杂度高、空间复杂度高

- 无法给出数据的内在含义

## 8.2 Scope of application

数值型和标称型



## 8.3 Theory

存在**带标签**的训练样本集，输入没有标签的新数据后，`新数据`的每个特征与`样本集`中数据对应的特征进行比较，然后提取样本集中特征最相似的分类的标签。一般只选择前k个最相似的数据，所以叫做k-邻近算法。

>  example：
>
> 确定一个电影是动作片还是爱情片，已知数据集特征：打斗场面次数、接吻镜头次数。给出新的未标签的电影，计算该电影与数据集中每个电影的距离，然后排序取前k个，通过分析k个电影的种类来确定新的未标签的电影类型。



# 9. Naive Bayes



# 10. CART



# Other

## Difference of C4.5/ID3/CART

- ID3决策树利用信息熵增益来划分结点

- C4.5决策树：先算信息熵增益，然后取增益率最高的
- CART决策树(Classification And Regression Tree)：可用于分类和回归